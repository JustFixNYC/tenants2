version: 2
jobs:
  lint:
    # Aside from running linters and other things that don't measure
    # code coverage, this step also writes to our caches, because it's
    # expected to take less time to run than the 'test' step that's
    # run in parallel with it. This allows the two steps to
    # collectively reduce the total test time as much as possible.
    working_directory: ~/tenants2
    docker:
      - image: justfixnyc/tenants2_base:0.10
        environment: &env
          PIPENV_VENV_IN_PROJECT: true
          DEBUG: yup
          ENABLE_WEBPACK_CONTENT_HASH: yup
          GIT_LFS_SKIP_SMUDGE: 1
    steps:
      - checkout
      - restore_cache:
          name: Restore Git LFS cache
          key: gitlfs-build
      - shell:
          name: Pull Git LFS files
          command: git lfs pull
      - save_cache:
          name: Save Git LFS cache
          key: gitlfs-build
          paths:
            # We need to save the Git LFS cache directory to save bandwidth, because GitHub only
            # allows for 1 GB download of anything stored in Git LFS per month.
            - .git/lfs
      - restore_cache:
          &restore_pipenv_cache # https://circleci.com/docs/2.0/caching/#pip-python
          name: Restore Pipenv cache
          keys:
            # when lock file changes, use increasingly general patterns to restore cache
            - pip-packages-v1-{{ .Branch }}-{{ checksum "Pipfile.lock" }}-{{ checksum "requirements.production.txt" }}
            - pip-packages-v1-{{ .Branch }}-{{ checksum "Pipfile.lock" }}
            - pip-packages-v1-{{ .Branch }}-
            - pip-packages-v1-
      - run: &install_python_deps
          name: Install Python dependencies
          command: |
            pipenv sync --dev
            pipenv run pip install -r requirements.production.txt
      - save_cache:
          name: Save Pipenv cache
          key: pip-packages-v1-{{ .Branch }}-{{ checksum "Pipfile.lock" }}-{{ checksum "requirements.production.txt" }}
          paths:
            - ".venv"
      - restore_cache:
          &restore_yarn_cache # https://circleci.com/docs/2.0/caching/#yarn-node
          name: Restore Yarn cache
          keys:
            # when lock file changes, use increasingly general patterns to restore cache
            - yarn-packages-v2-{{ .Branch }}-{{ checksum "yarn.lock" }}
            - yarn-packages-v2-{{ .Branch }}-
            - yarn-packages-v2-
      - run: &install_node_deps
          name: Install Node dependencies
          command: |
            # Print out yarn's cache dir for debugging purposes.
            yarn cache dir

            # This will ensure that 'npm prepare' scripts on dependencies are run.
            # For more details, see: https://stackoverflow.com/a/52767310
            yarn config set unsafe-perm true

            yarn install --frozen-lockfile
      - save_cache:
          name: Save Yarn cache
          paths:
            - /usr/local/share/.cache/yarn
          key: yarn-packages-v2-{{ .Branch }}-{{ checksum "yarn.lock" }}
      - run:
          name: Run linters
          command: |
            yarn build
            yarn lint
            pipenv run "yarn" l10n:extract-and-check
            pipenv run "black" --check .
            pipenv run "flake8"
            pipenv run "mypy" .
  test:
    working_directory: ~/tenants2
    docker:
      - image: justfixnyc/tenants2_base:0.10
        environment:
          <<: *env
          DATABASE_URL: postgis://justfix@localhost/justfix
          CC_TEST_REPORTER_ID: 0b47f78787493d017e97f3f141ab138e9188d1ebbe149bb0f28a8ff3314dfdd7
          USE_LAMBDA_HTTP_SERVER: yup
      - image: mdillon/postgis:10-alpine
        environment:
          POSTGRES_DB: justfix
          POSTGRES_USER: justfix
    steps:
      - checkout
      - restore_cache:
          name: Restore Git LFS cache
          key: gitlfs-build
      - shell:
          name: Pull Git LFS files
          command: git lfs pull
      - restore_cache:
          <<: *restore_pipenv_cache
      - run:
          <<: *install_python_deps
      - restore_cache:
          <<: *restore_yarn_cache
      - run:
          <<: *install_node_deps
      - run:
          name: CodeClimate before-build
          command: |
            curl -L https://codeclimate.com/downloads/test-reporter/test-reporter-latest-linux-amd64 > ./cc-test-reporter
            chmod +x ./cc-test-reporter
            ./cc-test-reporter before-build
      - run:
          name: Run tests
          command: |
            node --version

            # These are intended as smoke tests.
            node querybuilder.js --version
            node commondatabuilder.js --version

            yarn build
            yarn test --runInBand
            pipenv run "pytest" --cov=. --cov-report xml:./coverage/python/coverage.xml
      - run:
          name: CodeClimate combine and upload coverage
          command: |
            # Format the various coverage reports
            ./cc-test-reporter format-coverage -t lcov -o coverage/codeclimate.jest.json coverage/jest/lcov.info
            ./cc-test-reporter format-coverage -t coverage.py -o coverage/codeclimate.python.json coverage/python/coverage.xml
            # Combine the test coverage reports
            ./cc-test-reporter sum-coverage coverage/codeclimate.*.json
            # Attempt to submit the coverage report, but don't fail the build if this fails (`|| true`)
            ./cc-test-reporter upload-coverage --debug || true
      - store_test_results:
          path: test-results
      - store_artifacts:
          path: test-results
          destination: tr1
  deploy:
    working_directory: ~/tenants2_deploy
    docker:
      - image: circleci/python:3.8
        environment:
          GIT_LFS_SKIP_SMUDGE: 1
    steps:
      # Ideally we would use Docker Layer Caching (DLC) here to speed things up,
      # but apparently that's a premium feature:
      #
      #   https://circleci.com/docs/2.0/docker-layer-caching/
      - setup_remote_docker
      - run:
          name: Install Git LFS
          command: |
            curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo env os=debian dist=bullseye bash
            sudo apt-get install git-lfs
            git lfs install
      - checkout
      - restore_cache:
          key: gitlfs-deploy
      - shell:
          name: Pull Git LFS files
          command: git lfs pull
      - save_cache:
          key: gitlfs-deploy
          paths:
            # We need to save the Git LFS cache directory to save bandwidth, because GitHub only
            # allows for 1 GB download of anything stored in Git LFS per month.
            - .git/lfs
      - run:
          name: Deploy
          command: |
            # Note that you will need to set HEROKU_API_KEY in the CircleCI
            # settings for this to work. You can generate a Heroku API key
            # from the command-line with `heroku authorizations:create` for
            # production apps or `heroku auth:token` for development.
            curl https://cli-assets.heroku.com/install.sh | sh
            if [[ "${CIRCLE_BRANCH}" == "production" ]]; then
              # This should be the Heroku app name of our production instance.
              heroku git:remote -a tenants2

              # We actually want to use the development instance as our cache,
              # because it's very likely we're just deploying the same image
              # it's already running.
              export CACHE_FROM=registry.heroku.com/tenants2-dev/web:latest
            else
              # This should be the Heroku app name of our development instance.
              heroku git:remote -a tenants2-dev
              export CACHE_FROM=self
            fi
            python deploy.py selfcheck
            python deploy.py heroku -r heroku --cache-from=${CACHE_FROM}
workflows:
  version: 2
  test_and_deploy:
    jobs:
      - lint:
          filters:
            branches:
              ignore:
                # This is a bit counter-intuitive, but we've been extremely
                # diligent about only pushing to production once something
                # has been verified to pass CI on master. Because of this,
                # it's wasted effort to run the exact same tests on production
                # when we push to it, particularly since the merge on
                # production is a fast-forward merge and therefore represents
                # the exact same code that's already been tested on master.
                - production
      - test:
          filters:
            branches:
              ignore:
                # See above note on why we're ignoring production here.
                - production
      - deploy:
          requires:
            - test
            - lint
          filters:
            branches:
              only:
                - master
  only_deploy:
    jobs:
      - deploy:
          filters:
            branches:
              only:
                - production
